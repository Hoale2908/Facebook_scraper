{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hoale2908/Facebook_scraper/blob/main/FB_Crawler_Hoa_Le.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3uRmwV1aWob"
      },
      "source": [
        "# 0. Preparation\n",
        "- Using Chrome web browser\n",
        "- Install the Chrome extension \"Get cookies.txt\" (can be found [here](https://chrome.google.com/webstore/detail/get-cookiestxt/bgaddhkoddajcdgocldbbfleckgcbcid?hl=en)).\n",
        "-  Sign in your account on Facebook using Chrome. \n",
        "- Click on the extension and then \"Extract\" to save the .txt file to your computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on0ggamlaWoj"
      },
      "source": [
        "# 1. Install Library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this block and wait (about 1 min) for the installation to finish. "
      ],
      "metadata": {
        "id": "dWmUd7XoSfBT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEkUSyGsDC14",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "%pip install facebook_scraper\n",
        "from pathlib import Path\n",
        "print(\"Setup completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXWfsQnjaWol"
      },
      "source": [
        "# 2. Set up working folder and Upload cookies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the file .txt that you have downloaded using the \"Get cookies.txt\" extension."
      ],
      "metadata": {
        "id": "lz96ksceSMoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Create a folder\n",
        "FOLDER_NAME = \"Facebook_scraping\"\n",
        "FOLDER_PATH = \"/content/\" + FOLDER_NAME + \"/\"\n",
        "\n",
        "import os \n",
        "os.mkdir(FOLDER_PATH)\n",
        "\n",
        "# Upload the cookies\n",
        "os.chdir(FOLDER_PATH)\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Get path to cookies\n",
        "COOKIE_PATH = FOLDER_PATH + (os.listdir(FOLDER_PATH)[0])\n",
        "\n",
        "print('\\n \\n Completed.')"
      ],
      "metadata": {
        "id": "CSf0Uk12RATC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3g7SfrQaWon"
      },
      "source": [
        "# 3. Provide inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open the Facebook page on the web browser. Copy the remaining part of the URL starting after the \"facebook.com/\". \n",
        "\n",
        "For example: The full URL for CNN's page is \"https://www.facebook.com/cnn\", copy the string \"cnn\"."
      ],
      "metadata": {
        "id": "e8kUhbUSQTXn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqfpmr97DIJV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title \n",
        "# input the Page ID\n",
        "FANPAGE_LINK = input(\"Handle of the target page:\")\n",
        "\n",
        "# estimate the number of posts, then divide it by 4\n",
        "no_of_post = input('Please estimate the number of posts to be scraped:')\n",
        "PAGES_NUMBER = round(int(no_of_post) / 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYCY2iEVaWop"
      },
      "source": [
        "# 4. Scrape the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start scraping the data. When finished, all posts on the page will be saved to an Excel file named \"All_posts.xlsx\" and downloaded to your computer (in the default folder for downloads)."
      ],
      "metadata": {
        "id": "GN1ZM0eoRJV0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdrehHenDKyq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from facebook_scraper import get_posts\n",
        "import pandas as pd\n",
        "\n",
        "post_list = []\n",
        "for post in get_posts(FANPAGE_LINK, \n",
        "                    options={\"comments\": True, \"reactions\": True, \"allow_extra_requests\": True}, \n",
        "                    extra_info=True, pages=PAGES_NUMBER, cookies=COOKIE_PATH):\n",
        "    post_list.append(post)\n",
        "\n",
        "# Export all posts on the page to an Excel file\n",
        "df_allpost = pd.DataFrame(post_list)\n",
        "df_allpost.to_excel(\"All_posts.xlsx\")\n",
        "files.download(\"All_posts.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the file is generated and downloaded to your computer, open it and find the ID of the post that you want to scrape the comments. You will need to do this for each post that you want to scrape comments. \n",
        "\n",
        "After running the below code, the comments will be saved to an Excel file ended with \"_Comments_on_post\" and prefixed by the post ID."
      ],
      "metadata": {
        "id": "U49soTPoRPVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title \n",
        "# Get the post ID\n",
        "target_post_id = input(\"ID of the post that you want to scrape its comments (you can find it in the file All_posts.xlsx):\")\n",
        "\n",
        "# Get the full comments of such post\n",
        "index_row = df_allpost.index[df_allpost['post_id'] == target_post_id].tolist()[0]\n",
        "target_post_comment = df_allpost.loc[index_row,\"comments_full\"]\n",
        "\n",
        "# Export all the comments to the target post\n",
        "df_target_post_comment = pd.DataFrame(target_post_comment)\n",
        "df_target_post_comment.to_excel(target_post_id + \"_comments_on_post.xlsx\")\n",
        "files.download(target_post_id + \"_comments_on_post.xlsx\")"
      ],
      "metadata": {
        "id": "GF5i7FC1kayY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now as all the comments on the post have been scraped, you can continue to scrape all the replies to each comment if you wish to by running the below code. Replies will be saved to separate Excel files with the Comment ID on the filename. E.g. Replies to 10000222222.xlsx"
      ],
      "metadata": {
        "id": "k2uXL5zcTzor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# For each comment that has replies, we need to create a df for the replies\n",
        "# Iterating thru the comments of the target post; check if it has replies, if yes: create a df of the replies and save to an Excel file.\n",
        "for c in target_post_comment:\n",
        "    if (len(c['replies'])) > 0:\n",
        "        rep_df = pd.DataFrame(c['replies'])      \n",
        "        rep_df.to_excel(c['comment_id'] + \"_replies_to_comment.xlsx\")\n",
        "        files.download(c['comment_id'] + \"_replies_to_comment.xlsx\")"
      ],
      "metadata": {
        "id": "XIc67aKAamLx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}